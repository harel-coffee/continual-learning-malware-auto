#!/usr/bin/env python3
import argparse
import os
import numpy as np
import time
import torch
from torch import optim
import visual_plt
import utils
import pandas as pd
from param_stamp import get_param_stamp, get_param_stamp_from_args
import evaluate
from data import get_multitask_experiment, get_malware_multitask_experiment
from encoder import Classifier
from vae_models import AutoEncoder
import callbacks as cb
from train import train_cl
from continual_learner import ContinualLearner
from exemplars import ExemplarHandler
from replayer import Replayer
from param_values import set_default_values
from sklearn.metrics import f1_score, roc_auc_score
import copy


orig_feats_length, target_feats_length = 2492, 2500
dataset_name = 'drebin'
scenario = 'task'

# Use cuda?
cuda = torch.cuda.is_available()
device = torch.device("cuda" if cuda else "cpu")
print("CUDA is {}used".format("" if cuda else "NOT(!!) "))


# Prepare data for chosen experiment
print("\nPreparing the data...")
(train_datasets, test_datasets), config, classes_per_task = get_malware_multitask_experiment(
name='splitMNIST', dataset_name=dataset_name, scenario=scenario, orig_feats_length=orig_feats_length,\
target_feats_length=target_feats_length, tasks=6,
verbose=True, exception=True,
)

# config

model = Classifier(
    image_size=50, image_channels=1, classes=3,
    fc_bn=True, excit_buffer=False,).to(device)

model_save_path = './drebin_saved_models/6_class.pt'
model.load_state_dict(torch.load(model_save_path))


def validate(model, dataset, task_classes, batch_size=16, test_size=1024, verbose=True, allowed_classes=None,
             with_exemplars=False, no_task_mask=False, task=None):
    '''Evaluate precision (= accuracy or proportion correct) of a classifier ([model]) on [dataset].

    [allowed_classes]   None or <list> containing all "active classes" between which should be chosen
                            (these "active classes" are assumed to be contiguous)'''

    # Set model to eval()-mode
    mode = model.training
    model.eval()

    # Apply task-specifc "gating-mask" for each hidden fully connected layer (or remove it!)
    if hasattr(model, "mask_dict") and model.mask_dict is not None:
        if no_task_mask:
            model.reset_XdGmask()
        else:
            model.apply_XdGmask(task=task)

    # Loop over batches in [dataset]
    data_loader = utils.get_data_loader(dataset, batch_size, cuda=model._is_on_cuda())
    total_tested = total_correct = 0
    
    correct_labels = []
    predicted_labels = []
    y_predicts_scores = []
    
    normalized_scores = []
    
    
    #print(allowed_classes)
    for data, labels in data_loader:
        # -break on [test_size] (if "None", full dataset is used)
        if test_size:
            if total_tested >= test_size:
                break
        # -evaluate model (if requested, only on [allowed_classes])
        data, labels = data.to(model._device()), labels.to(model._device())
        labels = labels - allowed_classes[0] if (allowed_classes is not None) else labels
        print(labels)
        #print(allowed_classes)
        with torch.no_grad():
            if with_exemplars:
                predicted = model.classify_with_exemplars(data, allowed_classes=allowed_classes)
                # - in case of Domain-IL scenario, collapse all corresponding domains into same class
                if max(predicted).item() >= model.classes:
                    predicted = predicted % model.classes
            else:
                scores = model(data) if (allowed_classes is None) else model(data)[:, allowed_classes]
                #print(labels, scores)
                
                '''
                if get_valid_loss:
                    # Calculate prediction loss
                    if args.bce:
                        # -binary prediction loss
                        binary_targets = utils.to_one_hot(y.cpu(), y_hat.size(1)).to(y.device)
                        if self.binaryCE_distill and (scores is not None):
                            classes_per_task = int(y_hat.size(1) / task)
                            binary_targets = binary_targets[:, -(classes_per_task):]
                            binary_targets = torch.cat([torch.sigmoid(scores / self.KD_temp), binary_targets], dim=1)
                        predL = None if y is None else F.binary_cross_entropy_with_logits(
                            input=y_hat, target=binary_targets, reduction='none'
                        ).sum(dim=1).mean()     #--> sum over classes, then average over batch
                    else:
                        # -multiclass prediction loss
                        predL = None if y is None else F.cross_entropy(input=y_hat, target=y, reduction='mean')
                '''

                predicted_scores, predicted = torch.max(scores, 1)
                
                scores_copied = copy.deepcopy(scores)
                #print(task_classes)
                '''
                if task_classes > 2:
                    #score_detached = scores_copied.cpu().numpy()
                    
                    for detached_i in score_detached:
                        
                        if allowed_classes is not None:
                            pre_normal_i = [(float(k)-min(detached_i))/(max(detached_i)-min(detached_i)) for k in detached_i]
                            normal_i = [np.float(j)/sum(pre_normal_i) for j in pre_normal_i]
                            normalized_scores.append(np.array(normal_i))
                        else:
                            normalized_scores.append(np.array(detached_i))
                        
                        #print(sum(normal_i))
                       
                    #normalized_scores = np.array(normalized_scores, dtype=np.float32)
                    #print(roc_auc_score(labs, normalized_scores, multi_class="ovr", average="weighted"))
                    #print(scores, sum(score_detached[0]))
                    #print(roc_auc_score(labs, score_detached, multi_class="ovr", average="weighted"))
                    #y_predicts_scores.append((temp_scores))
                    y_predicts_scores += list(predicted.detach().cpu().numpy())
                else:
                    y_predicts_scores += list(predicted.detach().cpu().numpy())
                '''
                y_predicts_scores += list(predicted.detach().cpu().numpy())
                
                #print(roc_auc_score(np.array(list(labels.detach().cpu().numpy())),
                #                    np.array(list(scores.detach().cpu().numpy())),
                #                    multi_class="ovr", average="weighted"))
                #print(roc_auc_score(np.array(list(labels.detach().cpu().numpy())),\
                #                np.array(list(predicted.detach().cpu().numpy())), average="weighted"))
                
        # -update statistics
        total_correct += (predicted == labels).sum().item()
        total_tested += len(data)
        
        correct_labels += list(labels.cpu().numpy())
        predicted_labels += list(predicted.cpu().numpy())
        
    precision = total_correct / total_tested
    #y_predicts_scores = np.array(y_predicts_scores,dtype=np.float64)
    correct_labels = np.array(correct_labels)
    
    #print(y_predicts_scores)
    #normalized_scores = np.array(normalized_scores, dtype=np.float32)
    #print(len(normalized_scores))
    
    
    #print(len(correct_labels), np.unique(correct_labels), len(y_predicts_scores))
    
    #print(correct_labels, y_predicts_scores)
    f1score = f1_score(correct_labels, np.array(predicted_labels), average='weighted')
    print(f1score)
    #print(roc_auc_score(correct_labels, y_predicts_scores, average="weighted"))
    
    #rocscore = roc_auc_score(correct_labels, normalized_scores, multi_class="ovr", average="weighted")
    #rocscore = roc_auc_score(correct_labels, y_predicts_scores, average="weighted")
    #print(rocscore)
    
    
    #print(f'F1 score {f1score}')
    # Set model back to its initial mode, print result on screen (if requested) and return it
    #model.train(mode=mode)
    #if verbose:
    #     print('=> F1Score {:.3f}'.format(f1score))
    return f1score 

total_classes = 18
task = 6
task_classes = int(total_classes/task)
precs = [validate(
        model, test_datasets[i], task_classes, verbose=True, test_size=None, task=i+1, with_exemplars=False,
        allowed_classes=list(range(classes_per_task*i,\
        classes_per_task*(i+1))) if scenario=="task" else None) for i in range(task)]