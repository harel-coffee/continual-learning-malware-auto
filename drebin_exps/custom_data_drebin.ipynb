{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 7, 8, 9]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(range(6,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import ConcatDataset, Dataset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running data.py\n",
      " --> drebin: 3627 training and 898 testing samples\n",
      "[[0, 1], [2, 3], [4, 5], [6, 7], [8, 9], [10, 11], [12, 13], [14, 15], [16, 17]]\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import ConcatDataset, Dataset\n",
    "import torch\n",
    "\n",
    "\n",
    "def _permutate_image_pixels(image, permutation):\n",
    "    '''Permutate the pixels of an image according to [permutation].\n",
    "\n",
    "    [image]         3D-tensor containing the image\n",
    "    [permutation]   <ndarray> of pixel-indeces in their new order'''\n",
    "\n",
    "    if permutation is None:\n",
    "        return image\n",
    "    else:\n",
    "        c, h, w = image.size()\n",
    "        image = image.view(c, -1)\n",
    "        image = image[:, permutation]  #--> same permutation for each channel\n",
    "        image = image.view(c, h, w)\n",
    "        return image\n",
    "\n",
    "\n",
    "def get_dataset(name, type='train', download=True, capacity=None, permutation=None, dir='./datasets',\n",
    "                verbose=False, target_transform=None):\n",
    "    '''Create [train|valid|test]-dataset.'''\n",
    "\n",
    "    data_name = 'mnist' if name=='mnist28' else name\n",
    "    dataset_class = AVAILABLE_DATASETS[data_name]\n",
    "\n",
    "    # specify image-transformations to be applied\n",
    "    dataset_transform = transforms.Compose([\n",
    "        *AVAILABLE_TRANSFORMS[name],\n",
    "        transforms.Lambda(lambda x, p=permutation: _permutate_image_pixels(x, p)),\n",
    "    ])\n",
    "\n",
    "    # load data-set\n",
    "    dataset = dataset_class('{dir}/{name}'.format(dir=dir, name=data_name), train=False if type=='test' else True,\n",
    "                            download=download, transform=dataset_transform, target_transform=target_transform)\n",
    "\n",
    "    # print information about dataset on the screen\n",
    "    if verbose:\n",
    "        print(\" --> {}: '{}'-dataset consisting of {} samples\".format(name, type, len(dataset)))\n",
    "        #print(dataset)\n",
    "    # if dataset is (possibly) not large enough, create copies until it is.\n",
    "    if capacity is not None and len(dataset) < capacity:\n",
    "        dataset_copy = copy.deepcopy(dataset)\n",
    "        dataset = ConcatDataset([dataset_copy for _ in range(int(np.ceil(capacity / len(dataset))))])\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_malware_dataset(name, dir='../../', verbose=True):\n",
    "    '''Create [train|valid|test]-dataset.'''\n",
    "    \n",
    "    train_file = dir + 'NEW_drebin_train_all.npz'\n",
    "    test_file = dir + 'NEW_drebin_test_all.npz'\n",
    "    train_data = np.load(train_file)\n",
    "    test_data = np.load(test_file)\n",
    "\n",
    "    X_train, y_train = train_data['X_train'], train_data['y_train']\n",
    "    X_test, y_test = test_data['X_test'], test_data['y_test']    \n",
    "    \n",
    "    # print information about dataset on the screen\n",
    "    if verbose:\n",
    "        print(\" --> {}: {} training and {} testing samples\".format(name, len(y_train), len(y_test)))\n",
    "\n",
    "    return (X_train, y_train), (X_test, y_test)\n",
    "\n",
    "class malwareSubDataset(Dataset):\n",
    "    '''To sub-sample a dataset, taking only those samples with label in [sub_labels].\n",
    "\n",
    "    After this selection of samples has been made, it is possible to transform the target-labels,\n",
    "    which can be useful when doing continual learning with fixed number of output units.'''\n",
    "\n",
    "    def __init__(self, original_dataset, sub_labels, target_transform=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dataset, self.origlabels = original_dataset\n",
    "        self.sub_indeces = []\n",
    "        for index in range(len(self.dataset)):\n",
    "            if hasattr(original_dataset, \"targets\"):\n",
    "                if self.dataset.target_transform is None:\n",
    "                    label = self.dataset.targets[index]\n",
    "                else:\n",
    "                    label = self.dataset.target_transform(self.dataset.targets[index])\n",
    "            else:\n",
    "                label = self.dataset[index][1]\n",
    "            if label in sub_labels:\n",
    "                self.sub_indeces.append(index)\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sub_indeces)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sample = self.dataset[self.sub_indeces[index]]\n",
    "        target = self.origlabels[self.sub_indeces[index]]\n",
    "        #print((sample, target))\n",
    "        return (sample, target)\n",
    "    \n",
    "    \n",
    "def get_malware_multitask_experiment(name, dataset_name, scenario, tasks, data_dir=\"./datasets\", only_config=False, verbose=False,\n",
    "                             exception=False):\n",
    "    '''Load, organize and return train- and test-dataset for requested experiment.\n",
    "\n",
    "    [exception]:    <bool>; if True, for visualization no permutation is applied to first task (permMNIST) or digits\n",
    "                            are not shuffled before being distributed over the tasks (splitMNIST)'''\n",
    "\n",
    "    # depending on experiment, get and organize the datasets\n",
    "    if name == 'permMNIST':\n",
    "        # configurations\n",
    "        config = DATASET_CONFIGS['mnist']\n",
    "        classes_per_task = 10\n",
    "        if not only_config:\n",
    "            # prepare dataset\n",
    "            train_dataset = get_dataset('mnist', type=\"train\", permutation=None, dir=data_dir,\n",
    "                                        target_transform=None, verbose=verbose)\n",
    "            print(train_dataset.shape)\n",
    "            test_dataset = get_dataset('mnist', type=\"test\", permutation=None, dir=data_dir,\n",
    "                                       target_transform=None, verbose=verbose)\n",
    "            # generate permutations\n",
    "            if exception:\n",
    "                permutations = [None] + [np.random.permutation(config['size']**2) for _ in range(tasks-1)]\n",
    "            else:\n",
    "                permutations = [np.random.permutation(config['size']**2) for _ in range(tasks)]\n",
    "            # prepare datasets per task\n",
    "            train_datasets = []\n",
    "            test_datasets = []\n",
    "            for task_id, perm in enumerate(permutations):\n",
    "                target_transform = transforms.Lambda(\n",
    "                    lambda y, x=task_id: y + x*classes_per_task\n",
    "                ) if scenario in ('task', 'class') else None\n",
    "                train_datasets.append(TransformedDataset(\n",
    "                    train_dataset, transform=transforms.Lambda(lambda x, p=perm: _permutate_image_pixels(x, p)),\n",
    "                    target_transform=target_transform\n",
    "                ))\n",
    "                test_datasets.append(TransformedDataset(\n",
    "                    test_dataset, transform=transforms.Lambda(lambda x, p=perm: _permutate_image_pixels(x, p)),\n",
    "                    target_transform=target_transform\n",
    "                ))\n",
    "    elif name == 'splitMNIST':\n",
    "        if dataset_name == 'drebin':\n",
    "            num_class = 18\n",
    "        if dataset_name == 'andmal':\n",
    "            num_class = 50\n",
    "            \n",
    "        # check for number of tasks\n",
    "        if tasks>num_class:\n",
    "            raise ValueError(f\"Experiment 'splitMNIST' cannot have more than {num_class} tasks!\")\n",
    "            \n",
    "        # configurations\n",
    "        config = DATASET_CONFIGS['drebin']\n",
    "        \n",
    "        classes_per_task = int(np.floor(num_class / tasks))\n",
    "        \n",
    "        if not only_config:\n",
    "            # prepare permutation to shuffle label-ids (to create different class batches for each random seed)\n",
    "            permutation = np.array(list(range(num_class))) if exception else np.random.permutation(list(range(num_class)))\n",
    "            \n",
    "            target_transform = transforms.Lambda(lambda y, p=permutation: int(p[y]))\n",
    "            \n",
    "            # prepare train and test datasets with all classes\n",
    "            drebin_train, drebin_test = get_malware_dataset(dataset_name)\n",
    "            \n",
    "\n",
    "            # generate labels-per-task\n",
    "            labels_per_task = [\n",
    "                list(np.array(range(classes_per_task)) + classes_per_task * task_id) for task_id in range(tasks)\n",
    "            ]\n",
    "            \n",
    "            print(labels_per_task)\n",
    "            \n",
    "            # split them up into sub-tasks\n",
    "            train_datasets = []\n",
    "            test_datasets = []\n",
    "            for labels in labels_per_task:\n",
    "                target_transform = transforms.Lambda(\n",
    "                    lambda y, x=labels[0]: y - x\n",
    "                ) if scenario=='domain' else None\n",
    "                train_datasets.append(malwareSubDataset(drebin_train, labels, target_transform=target_transform))\n",
    "                test_datasets.append(malwareSubDataset(drebin_test, labels, target_transform=target_transform))\n",
    "            #print(f'here')\n",
    "            #print(test_datasets)\n",
    "    else:\n",
    "        raise RuntimeError('Given undefined experiment: {}'.format(name))\n",
    "\n",
    "    # If needed, update number of (total) classes in the config-dictionary\n",
    "    config['classes'] = classes_per_task if scenario=='domain' else classes_per_task*tasks\n",
    "\n",
    "    # Return tuple of train-, validation- and test-dataset, config-dictionary and number of classes per task\n",
    "    return config if only_config else ((train_datasets, test_datasets), config, classes_per_task)\n",
    "#----------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "\n",
    "class SubDataset(Dataset):\n",
    "    '''To sub-sample a dataset, taking only those samples with label in [sub_labels].\n",
    "\n",
    "    After this selection of samples has been made, it is possible to transform the target-labels,\n",
    "    which can be useful when doing continual learning with fixed number of output units.'''\n",
    "\n",
    "    def __init__(self, original_dataset, sub_labels, target_transform=None):\n",
    "        super().__init__()\n",
    "        self.dataset = original_dataset\n",
    "        self.sub_indeces = []\n",
    "        for index in range(len(self.dataset)):\n",
    "            if hasattr(original_dataset, \"targets\"):\n",
    "                if self.dataset.target_transform is None:\n",
    "                    label = self.dataset.targets[index]\n",
    "                else:\n",
    "                    label = self.dataset.target_transform(self.dataset.targets[index])\n",
    "            else:\n",
    "                label = self.dataset[index][1]\n",
    "            if label in sub_labels:\n",
    "                self.sub_indeces.append(index)\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sub_indeces)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sample = self.dataset[self.sub_indeces[index]]\n",
    "        if self.target_transform:\n",
    "            target = self.target_transform(sample[1])\n",
    "            sample = (sample[0], target)\n",
    "            print(sample)\n",
    "        return sample\n",
    "\n",
    "\n",
    "class ExemplarDataset(Dataset):\n",
    "    '''Create dataset from list of <np.arrays> with shape (N, C, H, W) (i.e., with N images each).\n",
    "\n",
    "    The images at the i-th entry of [exemplar_sets] belong to class [i], unless a [target_transform] is specified'''\n",
    "\n",
    "    def __init__(self, exemplar_sets, target_transform=None):\n",
    "        super().__init__()\n",
    "        self.exemplar_sets = exemplar_sets\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        total = 0\n",
    "        for class_id in range(len(self.exemplar_sets)):\n",
    "            total += len(self.exemplar_sets[class_id])\n",
    "        return total\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        total = 0\n",
    "        for class_id in range(len(self.exemplar_sets)):\n",
    "            exemplars_in_this_class = len(self.exemplar_sets[class_id])\n",
    "            if index < (total + exemplars_in_this_class):\n",
    "                class_id_to_return = class_id if self.target_transform is None else self.target_transform(class_id)\n",
    "                exemplar_id = index - total\n",
    "                break\n",
    "            else:\n",
    "                total += exemplars_in_this_class\n",
    "        image = torch.from_numpy(self.exemplar_sets[class_id][exemplar_id])\n",
    "        return (image, class_id_to_return)\n",
    "\n",
    "\n",
    "class TransformedDataset(Dataset):\n",
    "    '''Modify existing dataset with transform; for creating multiple MNIST-permutations w/o loading data every time.'''\n",
    "\n",
    "    def __init__(self, original_dataset, transform=None, target_transform=None):\n",
    "        super().__init__()\n",
    "        self.dataset = original_dataset\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        (input, target) = self.dataset[index]\n",
    "        if self.transform:\n",
    "            input = self.transform(input)\n",
    "        if self.target_transform:\n",
    "            target = self.target_transform(target)\n",
    "        return (input, target)\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "\n",
    "# specify available data-sets.\n",
    "AVAILABLE_DATASETS = {\n",
    "    'mnist': datasets.MNIST,\n",
    "}\n",
    "\n",
    "# specify available transforms.\n",
    "AVAILABLE_TRANSFORMS = {\n",
    "    'mnist': [\n",
    "        transforms.Pad(2),\n",
    "        transforms.ToTensor(),\n",
    "    ],\n",
    "    'mnist28': [\n",
    "        transforms.ToTensor(),\n",
    "    ],\n",
    "    'drebin': [\n",
    "        transforms.ToTensor(),\n",
    "    ],\n",
    "}\n",
    "\n",
    "\n",
    "# drebin --> 2492\n",
    "\n",
    "# specify configurations of available data-sets.\n",
    "DATASET_CONFIGS = {\n",
    "    'mnist': {'size': 32, 'channels': 1, 'classes': 10},\n",
    "    'mnist28': {'size': 28, 'channels': 1, 'classes': 10},\n",
    "    'drebin': {'size': 50, 'channels': 1, 'classes': 18},\n",
    "}\n",
    "\n",
    "\n",
    "print('running data.py')\n",
    "(train_datasets, test_datasets), config, classes_per_task = get_malware_multitask_experiment(\n",
    "    name='splitMNIST', dataset_name='drebin', scenario='class', tasks=9,\n",
    "    verbose=True, exception=True,\n",
    ")\n",
    "\n",
    "#print(test_datasets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 1], [2, 3], [4, 5], [6, 7], [8, 9], [10, 11], [12, 13], [14, 15], [16, 17]]\n"
     ]
    }
   ],
   "source": [
    "exception = False            \n",
    "permutation = np.array(list(range(18))) if exception else np.random.permutation(list(range(18)))\n",
    "\n",
    "target_transform = transforms.Lambda(lambda y, p=permutation: int(p[y]))\n",
    "\n",
    "# generate labels-per-task\n",
    "labels_per_task = [\n",
    "    list(np.array(range(2)) + 2 * task_id) for task_id in range(9)\n",
    "]\n",
    "\n",
    "print(labels_per_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " --> drebin: 3627 training and 898 testing samples\n"
     ]
    }
   ],
   "source": [
    "drebin_train, drebin_test = get_malware_dataset('drebin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = np.float32(drebin_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float32')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, origlabels = drebin_test\n",
    "sub_labels = labels_per_task[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_indeces = []\n",
    "for index in range(len(dataset)):\n",
    "    label = origlabels[index]\n",
    "    \n",
    "    if label in sub_labels:\n",
    "        sub_indeces.append(index)\n",
    "        \n",
    "target_transform = target_transform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class malwareSubDataset(Dataset):\n",
    "    '''To sub-sample a dataset, taking only those samples with label in [sub_labels].\n",
    "\n",
    "    After this selection of samples has been made, it is possible to transform the target-labels,\n",
    "    which can be useful when doing continual learning with fixed number of output units.'''\n",
    "    \n",
    "    # drebin dataset feature length --> 2492\n",
    "    \n",
    "    \n",
    "    def __init__(self, original_dataset, orig_length_features, target_length_features ,sub_labels, target_transform=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dataset, self.origlabels = original_dataset\n",
    "        self.orig_length_features = orig_length_features\n",
    "        self.target_length_features = target_length_features\n",
    "        \n",
    "        self.sub_indeces = []\n",
    "        for index in range(len(self.dataset)):\n",
    "            label = self.origlabels[index]\n",
    "            \n",
    "            if label in sub_labels:\n",
    "                self.sub_indeces.append(index)\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sub_indeces)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        self.padded_features = np.zeros(self.target_length_features - self.orig_length_features)\n",
    "        sample = np.concatenate((self.dataset[self.sub_indeces[index]],self.padded_features))\n",
    "        target = self.origlabels[self.sub_indeces[index]]\n",
    "        #print((sample, target))\n",
    "        return (sample, target)\n",
    "    \n",
    "task_dataset = malwareSubDataset(drebin_test, 2492, 2500, labels_per_task[2], target_transform=target_transform)    \n",
    "\n",
    "for i in range(10):\n",
    "    try:\n",
    "        dloader = iter(DataLoader(task_dataset, batch_size=32, collate_fn=default_collate, shuffle=True))\n",
    "        x, y = next(dloader) \n",
    "        print(len(x[0]))\n",
    "    except StopIteration:\n",
    "        dloader = iter(DataLoader(task_dataset, 32, shuffle=True, num_workers=1, collate_fn=default_collate))\n",
    "        inps, targets = next(self.batch_iterator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    try:\n",
    "        dloader = iter(DataLoader(task_dataset, batch_size=32, collate_fn=default_collate, shuffle=True))\n",
    "        x, y = next(dloader) \n",
    "        print(x)\n",
    "    except StopIteration:\n",
    "        dloader = iter(DataLoader(task_dataset, 32, shuffle=True, num_workers=1, collate_fn=default_collate))\n",
    "        inps, targets = next(self.batch_iterator)\n",
    "        #print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    try:\n",
    "        x, y = next(dloader) \n",
    "        print(y)\n",
    "    except StopIteration:\n",
    "    dloader = iter(DataLoader(task_dataset, 32, shuffle=True, num_workers=1, collate_fn=default_collate))\n",
    "    inps, targets = next(self.batch_iterator)        \n",
    "    \n",
    "    \n",
    "    try:\n",
    "    inps, targets = next(self.batch_iterator)\n",
    "except StopIteration:\n",
    "    self.batch_iterator = iter(data.DataLoader(self.train_dataset, self.args.batch_size, shuffle=True, num_workers=self.args.num_workers, collate_fn=detection_collate))\n",
    "    inps, targets = next(self.batch_iterator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "\n",
    "def get_data_loader(dataset, batch_size, cuda=False, collate_fn=None, drop_last=False, augment=False):\n",
    "    '''Return <DataLoader>-object for the provided <DataSet>-object [dataset].'''\n",
    "\n",
    "    # If requested, make copy of original dataset to add augmenting transform (without altering original dataset)\n",
    "    if augment:\n",
    "        dataset_ = copy.deepcopy(dataset)\n",
    "        dataset_.transform = transforms.Compose([dataset.transform, *data.AVAILABLE_TRANSFORMS['augment']])\n",
    "    else:\n",
    "        dataset_ = dataset\n",
    "\n",
    "    # Create and return the <DataLoader>-object\n",
    "    return DataLoader(\n",
    "        dataset_, batch_size=batch_size, shuffle=True,\n",
    "        collate_fn=(collate_fn or default_collate), drop_last=drop_last,\n",
    "        **({'num_workers': 0, 'pin_memory': True} if cuda else {})\n",
    "    )\n",
    "\n",
    "for task, test_dataset in enumerate(test_datasets, 1):\n",
    "    #print(task)\n",
    "    #data_loader = iter(get_data_loader(test_dataset, 32, cuda=False, drop_last=True))\n",
    "    DataLoader(test_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "# specify available data-sets.\n",
    "AVAILABLE_DATASETS = {\n",
    "    'mnist': datasets.MNIST,\n",
    "}\n",
    "\n",
    "# specify available transforms.\n",
    "AVAILABLE_TRANSFORMS = {\n",
    "    'mnist': [\n",
    "        transforms.Pad(2),\n",
    "        transforms.ToTensor(),\n",
    "    ],\n",
    "    'mnist28': [\n",
    "        transforms.ToTensor(),\n",
    "    ],\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "def get_dataset(name, type='train', download=True, capacity=None, permutation=None, dir='./datasets',\n",
    "                verbose=False, target_transform=None):\n",
    "    '''Create [train|valid|test]-dataset.'''\n",
    "\n",
    "    data_name = 'mnist' if name=='mnist28' else name\n",
    "    dataset_class = AVAILABLE_DATASETS[data_name]\n",
    "\n",
    "    # specify image-transformations to be applied\n",
    "    dataset_transform = transforms.Compose([\n",
    "        *AVAILABLE_TRANSFORMS[name],\n",
    "        transforms.Lambda(lambda x, p=permutation: _permutate_image_pixels(x, p)),\n",
    "    ])\n",
    "\n",
    "    # load data-set\n",
    "    dataset = dataset_class('{dir}/{name}'.format(dir=dir, name=data_name), train=False if type=='test' else True,\n",
    "                            download=download, transform=dataset_transform, target_transform=target_transform)\n",
    "\n",
    "    # print information about dataset on the screen\n",
    "    if verbose:\n",
    "        print(\" --> {}: '{}'-dataset consisting of {} samples\".format(name, type, len(dataset)))\n",
    "        #print(dataset)\n",
    "    # if dataset is (possibly) not large enough, create copies until it is.\n",
    "    if capacity is not None and len(dataset) < capacity:\n",
    "        dataset_copy = copy.deepcopy(dataset)\n",
    "        dataset = ConcatDataset([dataset_copy for _ in range(int(np.ceil(capacity / len(dataset))))])\n",
    "\n",
    "    return dataset\n",
    "\n",
    "test_dataset = get_dataset('mnist', type=\"test\", permutation=None, dir='./CL_from_Ven/msr_cl/datasets',\n",
    "                                       target_transform=None, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 10000\n",
       "    Root location: ./CL_from_Ven/msr_cl/datasets/mnist\n",
       "    Split: Test\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               Pad(padding=2, fill=0, padding_mode=constant)\n",
       "               ToTensor()\n",
       "               Lambda()\n",
       "           )"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cnt = 0\n",
    "for idx, i in enumerate(test_dataset):\n",
    "    #print(idx, i)\n",
    "    #print(type(i[0][0]))\n",
    "    print(i[0][0].dtype)\n",
    "    if cnt == 1:\n",
    "        break\n",
    "    cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "\n",
    "dloader = iter(DataLoader(test_dataset, 32, shuffle=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _permutate_image_pixels(image, permutation):\n",
    "    '''Permutate the pixels of an image according to [permutation].\n",
    "\n",
    "    [image]         3D-tensor containing the image\n",
    "    [permutation]   <ndarray> of pixel-indeces in their new order'''\n",
    "\n",
    "    if permutation is None:\n",
    "        return image\n",
    "    else:\n",
    "        c, h, w = image.size()\n",
    "        image = image.view(c, -1)\n",
    "        image = image[:, permutation]  #--> same permutation for each channel\n",
    "        image = image.view(c, h, w)\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "for i in range(2):    \n",
    "    x, y = next(dloader) \n",
    "    print(type(x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
