{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## from CADE paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(1)\n",
    "\n",
    "import os, sys\n",
    "import logging\n",
    "import numpy as np\n",
    "import time\n",
    "from timeit import default_timer as timer\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from collections import OrderedDict, Counter\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_features(dataset, newfamily, folder='data/'):\n",
    "    logging.info('Loading ' + dataset + ' feature vectors and labels...')\n",
    "    filepath = os.path.join(folder, dataset + '.npz')\n",
    "    data = np.load(filepath)\n",
    "    X_train, y_train, X_test, y_test = data['X_train'], data['y_train'], data['X_test'], data['y_test']\n",
    "\n",
    "    logging.debug(f'before label adjusting: y_train: {Counter(y_train)}\\n  y_test: {Counter(y_test)}')\n",
    "\n",
    "    if 'drebin' in dataset:\n",
    "        PERSISTENT_NEW_FAMILY = 7\n",
    "    elif 'IDS' in dataset:\n",
    "        PERSISTENT_NEW_FAMILY = 3\n",
    "    elif 'bluehex' in dataset:\n",
    "        PERSISTENT_NEW_FAMILY = newfamily\n",
    "    else:\n",
    "        logging.error(f'dataset {dataset} not supported')\n",
    "        sys.exit(-4)\n",
    "\n",
    "    '''transform training set to continuous labels, always use the biggest label as the unseen family'''\n",
    "    le = LabelEncoder()\n",
    "    y_train_prime = le.fit_transform(y_train)\n",
    "    mapping = {}\n",
    "    for i in range(len(y_train)):\n",
    "        mapping[y_train[i]] = y_train_prime[i]  # mapping: real label -> converted label\n",
    "\n",
    "    logging.debug(f'LabelEncoder mapping: {mapping}')\n",
    "\n",
    "    y_test_prime = np.zeros(shape=y_test.shape, dtype=np.int32)\n",
    "    for i in range(len(y_test)):\n",
    "        if y_test[i] not in y_train:  # new family\n",
    "            y_test_prime[i] = PERSISTENT_NEW_FAMILY\n",
    "        else:\n",
    "            y_test_prime[i] = mapping[y_test[i]]\n",
    "\n",
    "    y_train_prime = np.array(y_train_prime, dtype=np.int32)\n",
    "    logging.debug(f'after relabeling training: {Counter(y_train_prime)}')\n",
    "    logging.debug(f'after relabeling testing: {Counter(y_test_prime)}')\n",
    "\n",
    "    return X_train, y_train_prime, X_test, y_test_prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folder(name):\n",
    "    if not os.path.exists(name):\n",
    "        os.makedirs(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = 'feature_vectors/'\n",
    "family_file = 'sha256_family.csv'\n",
    "drebin_metadata = 'drebin_metadata.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-3-64faacfaaef1>, line 30)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-64faacfaaef1>\"\u001b[0;36m, line \u001b[0;32m30\u001b[0m\n\u001b[0;31m    saved_file = os.path.join(intermediate_folder, f'drebin_family_{given_family}_sha_timestamp_family.csv')\u001b[0m\n\u001b[0m                                                                                                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def sort_drebin_family_by_time(intermediate_folder, given_family):\n",
    "    '''\n",
    "    sort the families of top 8 (excluding Opfake because Opfake and FakeInstaller are confusing) by\n",
    "    timestamp and saved to a new file, according to \"latest_modify_time\"\n",
    "    also return the sha list of the new family (the left from the top 8)\n",
    "    '''\n",
    "    top8 = ['FakeInstaller', 'DroidKungFu', 'Plankton',\n",
    "            'GinMaster', 'BaseBridge',\n",
    "            'Iconosys', 'Kmin', 'FakeDoc']\n",
    "\n",
    "    sha_family_dict = {}\n",
    "    sha_timestamp_dict = {}\n",
    "\n",
    "\n",
    "    with open('drebin_metadata.csv', 'r') as f:\n",
    "        next(f)\n",
    "        for line in f:\n",
    "            sha, family, latest_modify_time = line.strip().split(',')\n",
    "            if family in top8:\n",
    "                family_int = top8.index(family)\n",
    "                if family_int == given_family and latest_modify_time != 'None':\n",
    "                    sha_family_dict[sha] = family_int\n",
    "                    sha_timestamp_dict[sha] = datetime.strptime(latest_modify_time, \"%Y-%m-%d %H:%M:%S\")\n",
    "                    sha_timestamp_dict = OrderedDict(sorted(sha_timestamp_dict.items(),\n",
    "                                                                key=lambda x: x[1], reverse=False))\n",
    "\n",
    "\n",
    "    sha_sorted_by_time = []\n",
    "    label_sorted_by_time = []\n",
    "    saved_file = os.path.join(intermediate_folder, f'drebin_family_{given_family}_sha_timestamp_family.csv')\n",
    "    with open(saved_file, 'w') as f:\n",
    "        f.write('sha256,timestamp,family\\n')\n",
    "        for sha, ts in sha_timestamp_dict.items():\n",
    "            sha_sorted_by_time.append(sha)\n",
    "            label_sorted_by_time.append(sha_family_dict[sha])\n",
    "            f.write(f'{sha},{ts},{sha_family_dict[sha]}\\n')\n",
    "\n",
    "    return sha_sorted_by_time, label_sorted_by_time\n",
    "\n",
    "\n",
    "save_folder = 'drebin_processed'\n",
    "create_folder(save_folder)\n",
    "saved_data_file = os.path.join(save_folder, f'{dataset_name}.npz')\n",
    "if os.path.exists(saved_data_file):\n",
    "    logging.info(f'{saved_data_file} exists, no need to re-generate')\n",
    "else:\n",
    "    '''Train fit test, sort by timestamp, samples do not have timestamp would be removed'''\n",
    "    logging.info('Preparing Drebin malware data...')\n",
    "    raw_feature_vectors_folder = root_dir\n",
    "\n",
    "    intermediate_folder = os.path.join(save_folder, 'family_based') # for saving intermediate data files.\n",
    "    create_folder(intermediate_folder)\n",
    "    \n",
    "    top8 = ['FakeInstaller', 'DroidKungFu', 'Plankton',\n",
    "            'GinMaster', 'BaseBridge',\n",
    "            'Iconosys', 'Kmin', 'FakeDoc']  \n",
    "\n",
    "    sha_sorted_by_time, label_sorted_by_time =\\\n",
    "                    sort_drebin_family_by_time(intermediate_folder, '0')\n",
    "    logging.debug(f'sha_sorted_by_time len: {len(sha_sorted_by_time)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        \n",
    "def prepare_drebin_data(dataset_name, folder='data/', test_ratio=0.1, newfamily=7):\n",
    "    saved_data_file = os.path.join(folder, f'{dataset_name}.npz')\n",
    "    if os.path.exists(saved_data_file):\n",
    "        logging.info(f'{saved_data_file} exists, no need to re-generate')\n",
    "    else:\n",
    "        '''Train fit test, use only 7 of top 8 families, sort by timestamp, samples do not have timestamp would be removed'''\n",
    "        logging.info('Preparing Drebin malware data...')\n",
    "        raw_feature_vectors_folder = 'feature_vectors/'\n",
    "\n",
    "        intermediate_folder = os.path.join('./', dataset_name) # for saving intermediate data files.\n",
    "        create_folder(intermediate_folder)\n",
    "        sha_sorted_by_time, label_sorted_by_time, newfamily_sha_list =\\\n",
    "                        sort_drebin_7family_by_time(intermediate_folder, newfamily)\n",
    "        logging.debug(f'sha_sorted_by_time len: {len(sha_sorted_by_time)}')\n",
    "\n",
    "        '''split 8 families to training and testing set by timestamp, insert the new family to the testing set'''\n",
    "        train_shas, test_shas, train_labels, test_labels = split_drebin_train_and_test(sha_sorted_by_time,\n",
    "                                                                                    label_sorted_by_time,\n",
    "                                                                                    newfamily_sha_list,\n",
    "                                                                                    test_ratio,\n",
    "                                                                                    newfamily)\n",
    "\n",
    "        '''get all the feature names in the training set'''\n",
    "        train_feature_names = get_training_full_feature_names(intermediate_folder, newfamily,\n",
    "                                                            raw_feature_vectors_folder, train_shas)\n",
    "\n",
    "        '''save all the training set feature vectors'''\n",
    "        saved_train_vectors = save_training_full_feature_vectors(intermediate_folder, raw_feature_vectors_folder,\n",
    "                                                                train_shas, train_feature_names, train_labels, newfamily)\n",
    "\n",
    "        '''feature selection on the training set'''\n",
    "        selected_features, saved_selected_vectors_file = get_selected_features(intermediate_folder, saved_train_vectors,\n",
    "                                                                            newfamily, train_feature_names)\n",
    "\n",
    "        ''' generate the final data by saving feature vectors of both training and testing set'''\n",
    "        samples = len(test_shas)\n",
    "        feas = len(selected_features)\n",
    "        selected_features = list(selected_features)  # numpy array does not have index method\n",
    "        X_test = np.zeros((samples, feas))\n",
    "        for sample_idx, sha in enumerate(test_shas):\n",
    "            sys.stdin = open(f'{raw_feature_vectors_folder}/{sha}')\n",
    "            lines = sys.stdin.readlines()\n",
    "            for idx, l in enumerate(lines):\n",
    "                if l != '\\n':\n",
    "                    try:\n",
    "                        fea_idx = selected_features.index(l.strip())\n",
    "                        X_test[sample_idx][fea_idx] = 1\n",
    "                    except: # ignore unseen features.\n",
    "                        pass\n",
    "\n",
    "        y_test = np.array([int(label) for label in test_labels])\n",
    "        train_data = np.load(saved_selected_vectors_file)\n",
    "        X_train, y_train = train_data['X_train'], train_data['y_train']\n",
    "        logging.info(f'X_train: {X_train.shape}, y_train: {y_train.shape}')\n",
    "        logging.info(f'X_test: {X_test.shape}, y_test: {y_test.shape}')\n",
    "        np.savez_compressed(saved_data_file,\n",
    "                            X_train=X_train, y_train=y_train,\n",
    "                            X_test=X_test, y_test=y_test)\n",
    "        for idx, x in enumerate(X_test):\n",
    "            if np.all(x == 0):\n",
    "                logging.warning(f'X_test {idx} all 0')\n",
    "\n",
    "        logging.info('Preparing Drebin malware data finished')\n",
    "\n",
    "        \n",
    "def sort_drebin_7family_by_time(intermediate_folder, newfamily):\n",
    "    '''\n",
    "    sort the 7 families of top 8 (excluding Opfake because Opfake and FakeInstaller are confusing) by\n",
    "    timestamp and saved to a new file, according to \"latest_modify_time\"\n",
    "    also return the sha list of the new family (the left from the top 8)\n",
    "    '''\n",
    "    top8 = ['FakeInstaller', 'DroidKungFu', 'Plankton',\n",
    "            'GinMaster', 'BaseBridge',\n",
    "            'Iconosys', 'Kmin', 'FakeDoc']\n",
    "\n",
    "    sha_family_dict = {}\n",
    "    sha_timestamp_dict = {}\n",
    "    newfamily_sha_list = []\n",
    "    newfamily_sha_timestamp_dict = {}\n",
    "\n",
    "    with open('drebin_metadata.csv', 'r') as f:\n",
    "        next(f)\n",
    "        for line in f:\n",
    "            sha, family, latest_modify_time = line.strip().split(',')\n",
    "            if family in top8:\n",
    "                family_int = top8.index(family)\n",
    "                if family_int == newfamily:\n",
    "                    newfamily_sha_list.append(sha)\n",
    "                    if latest_modify_time != 'None':\n",
    "                        newfamily_sha_timestamp_dict[sha] = datetime.strptime(latest_modify_time, \"%Y-%m-%d %H:%M:%S\")\n",
    "                        newfamily_sha_timestamp_dict = OrderedDict(sorted(newfamily_sha_timestamp_dict.items(),\n",
    "                                                                        key=lambda x: x[1], reverse=False))\n",
    "                else:\n",
    "                    if latest_modify_time != 'None':\n",
    "                        sha_family_dict[sha] = family_int\n",
    "                        sha_timestamp_dict[sha] = datetime.strptime(latest_modify_time, \"%Y-%m-%d %H:%M:%S\")\n",
    "                        sha_timestamp_dict = OrderedDict(sorted(sha_timestamp_dict.items(),\n",
    "                                                                key=lambda x: x[1], reverse=False))\n",
    "\n",
    "\n",
    "    sha_sorted_by_time = []\n",
    "    label_sorted_by_time = []\n",
    "    saved_file = os.path.join(intermediate_folder, f'drebin_new{newfamily}_sha_timestamp_family.csv')\n",
    "    with open(saved_file, 'w') as f:\n",
    "        f.write('sha256,timestamp,family\\n')\n",
    "        for sha, ts in sha_timestamp_dict.items():\n",
    "            sha_sorted_by_time.append(sha)\n",
    "            label_sorted_by_time.append(sha_family_dict[sha])\n",
    "            f.write(f'{sha},{ts},{sha_family_dict[sha]}\\n')\n",
    "        for sha, ts in newfamily_sha_timestamp_dict.items():\n",
    "            f.write(f'{sha},{ts},{newfamily}\\n')\n",
    "\n",
    "    return sha_sorted_by_time, label_sorted_by_time, newfamily_sha_list\n",
    "\n",
    "\n",
    "def split_drebin_train_and_test(sha_sorted_by_time, label_sorted_by_time, newfamily_sha_list, test_ratio, newfamily):\n",
    "    test_num = int(len(sha_sorted_by_time) * test_ratio)\n",
    "    train_shas = sha_sorted_by_time[0:-test_num]\n",
    "    train_labels = label_sorted_by_time[0:-test_num]\n",
    "    test_shas = sha_sorted_by_time[-test_num:] + newfamily_sha_list\n",
    "    test_labels = label_sorted_by_time[-test_num:] + [newfamily] * len(newfamily_sha_list)\n",
    "    logging.debug(f'train_shas: {len(train_shas)}, test_shas: {len(test_shas)}')\n",
    "\n",
    "    return train_shas, test_shas, train_labels, test_labels\n",
    "\n",
    "\n",
    "def get_training_full_feature_names(intermediate_folder, newfamily, raw_feature_vectors_folder, train_shas):\n",
    "    saved_train_feature_file = os.path.join(intermediate_folder, f'drebin_new{newfamily}_full_training_features.txt')\n",
    "    if os.path.exists(saved_train_feature_file):\n",
    "        train_feature_names = []\n",
    "        with open(saved_train_feature_file, 'r') as f:\n",
    "            for line in f:\n",
    "                train_feature_names.append(line.strip())\n",
    "    else:\n",
    "        train_feature_names = set()\n",
    "        for sha in train_shas:\n",
    "            sys.stdin = open(f'{raw_feature_vectors_folder}/{sha}')\n",
    "            lines = sys.stdin.readlines()\n",
    "            for l in lines:\n",
    "                if l != '\\n':\n",
    "                    train_feature_names.add(l.strip())\n",
    "\n",
    "        train_feature_names = sorted(list(train_feature_names))\n",
    "        logging.info(f'[drebin-new{newfamily}] # of features in training set: {len(train_feature_names)}')\n",
    "        with open(saved_train_feature_file, 'w') as f:\n",
    "            for fea in train_feature_names:\n",
    "                f.write(fea + '\\n')\n",
    "\n",
    "    return train_feature_names\n",
    "\n",
    "\n",
    "def save_training_full_feature_vectors(intermediate_folder, raw_feature_vectors_folder,\n",
    "                                       train_shas, train_feature_names, train_labels, newfamily):\n",
    "    saved_train_vectors = os.path.join(intermediate_folder, f'drebin_new{newfamily}_train_full_feature_vectors.npz')\n",
    "    if not os.path.exists(saved_train_vectors):\n",
    "        samples = len(train_shas)\n",
    "        feas = len(train_feature_names)\n",
    "        X = np.zeros((samples, feas))\n",
    "        for sample_idx, sha in enumerate(train_shas):\n",
    "            sys.stdin = open(f'{raw_feature_vectors_folder}/{sha}')\n",
    "            lines = sys.stdin.readlines()\n",
    "            for l in lines:\n",
    "                if l != '\\n':\n",
    "                    fea_idx = train_feature_names.index(l.strip())\n",
    "                    X[sample_idx][fea_idx] = 1\n",
    "\n",
    "        y = np.array([int(label) for label in train_labels])\n",
    "        np.savez_compressed(saved_train_vectors, X_train=X, y_train=y)\n",
    "\n",
    "    return saved_train_vectors\n",
    "\n",
    "\n",
    "def get_selected_features(intermediate_folder, saved_train_vectors, newfamily, train_feature_names):\n",
    "    train_data = np.load(saved_train_vectors)\n",
    "    X, y = train_data['X_train'], train_data['y_train']\n",
    "    logging.debug(f'[drebin_new_{newfamily}] before feature selection X shape: {X.shape}')\n",
    "    selector = VarianceThreshold(0.003)\n",
    "    X_select = selector.fit_transform(X)\n",
    "    logging.debug(f'[drebin_new_{newfamily}] after feature selection X_select shape: {X_select.shape}')\n",
    "\n",
    "    selected_feature_indices = selector.get_support(indices=True)\n",
    "    # logging.debug(f'selected_feature_indices: {list(selected_feature_indices)}')\n",
    "    selected_features = np.array(train_feature_names)[selected_feature_indices]\n",
    "\n",
    "    ''' save selected features and corresponding feature vectors of training set '''\n",
    "    saved_selected_feature_file = os.path.join(intermediate_folder, f'drebin_new{newfamily}_train_selected_features.txt')\n",
    "    if not os.path.exists(saved_selected_feature_file):\n",
    "        with open(saved_selected_feature_file, 'w') as fout:\n",
    "            for fea in selected_features:\n",
    "                fout.write(f'{fea}\\n')\n",
    "    saved_selected_vectors_file = os.path.join(intermediate_folder, f'drebin_new{newfamily}_train_selected_feature_vectors.npz')\n",
    "    if not os.path.exists(saved_selected_vectors_file):\n",
    "        np.savez_compressed(saved_selected_vectors_file, X_train=X_select, y_train=y)\n",
    "\n",
    "    return selected_features, saved_selected_vectors_file\n",
    "\n",
    "\n",
    "prepare_drebin_data('drebin', folder='./', test_ratio=0.1, newfamily=7)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## from Nate"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import os\n",
    "import time\n",
    "from random import random, shuffle\n",
    "import numpy as np\n",
    "\n",
    "def read_file(path):\n",
    "    \"\"\"\n",
    "    Loads a sample from the Drebing dataset.\n",
    "    ftypes is the category for each feature\n",
    "    features is the list of feature strings (with the category information removed from the string)\n",
    "    \"\"\"\n",
    "    with open(path, 'r') as fi:\n",
    "        ftypes = []\n",
    "        features = []\n",
    "        for line in fi:\n",
    "            if line.strip():\n",
    "                p = line.strip().split('::')\n",
    "                ftype = p[0]\n",
    "                feature = ' '.join(p[1:])\n",
    "                ftypes.append(ftype)\n",
    "                features.append(feature)\n",
    "    #features = [\"{}::{}\".format(ftypes[i], features[i]) for i in range(len(features))]\n",
    "    return ftypes, features\n",
    "\n",
    "def get_global_features_list(root_dir = 'feature_vectors/',\n",
    "                             filter_fnames = ()):\n",
    "    \"\"\"\n",
    "    Generate the list of unique feature strings present in the dataset\n",
    "    filter_fnames arg can be provided as a list of file names that will be ignored when including unique features. By default, all samples are used.\n",
    "    \"\"\"\n",
    "    # build list of feature files from files in directory\n",
    "    fname_list = []\n",
    "    for rt,drs,fnames in os.walk(root_dir):\n",
    "        fname_list.extend(fnames)\n",
    "    fname_list = set(fname_list)\n",
    "    # discard testing feature files (maybe?)\n",
    "    [fname_list.discard(fname) for fname in filter_fnames]\n",
    "    # build set of known unique features\n",
    "    unique_features = set()\n",
    "    for fname in fname_list:\n",
    "        ftypes, features = read_file(os.path.join(root_dir, fname))\n",
    "        # re-append the feature category information to the feature strings. Probably not necessary, but just in case...\n",
    "        features = [\"{}::{}\".format(ftypes[i], features[i]) for i in range(len(features))]\n",
    "        unique_features.update(features)\n",
    "    unique_features = sorted(list(unique_features))\n",
    "    return unique_features\n",
    "\n",
    "def data_generator(unique_features = (),\n",
    "                   fpath_list = (),\n",
    "                   batch_size = 32):\n",
    "    \"\"\"\n",
    "    Create a python generator that loads Drebin sample files in batches.\n",
    "    \"\"\"\n",
    "    assert len(fpath_list) > 0\n",
    "    batch = []\n",
    "    for fpath in fpath_list:\n",
    "        ftypes, features = read_file(fpath)\n",
    "        features = [\"{}::{}\".format(ftypes[i], features[i]) for i in range(len(features))]\n",
    "        fvector = np.array([1 if feature in features else 0 for feature in unique_features])\n",
    "        batch.append(fvector)\n",
    "        if len(batch) == batch_size:\n",
    "            yield np.array(batch)\n",
    "            batch = []\n",
    "    yield np.array(batch)\n",
    "\n",
    "def get_data_all(unique_features = (),\n",
    "                   fpath_list = ()):\n",
    "\n",
    "    assert len(fpath_list) > 0\n",
    "    all_data = []\n",
    "\n",
    "    for idx, fpath in enumerate(fpath_list):\n",
    "        ftypes, features = read_file(fpath)\n",
    "        features = [\"{}::{}\".format(ftypes[i], features[i]) for i in range(len(features))]\n",
    "        fvector = np.array([1 if feature in features else 0 for feature in unique_features])\n",
    "        all_data.append(fvector)\n",
    "        \n",
    "        if idx%10000 == 0:\n",
    "            print('done with ',idx, 'samples.')\n",
    "    \n",
    "    return np.array(all_data)\n",
    "\n",
    "def load_data(family_file = 'sha256_family.csv', root_dir = 'feature_vectors'):\n",
    "    \"\"\"\n",
    "    Example load_data function attempts to load all Drebin sample files.\n",
    "    Requires a huge amount of memory (~fails at 120k samples on my system)\n",
    "    \"\"\"\n",
    "    if os.path.exists('./unique_features.npy'):\n",
    "        unique_features = np.load('./unique_features.npy')\n",
    "    else:\n",
    "        # read through files and create the list of feature strings\n",
    "        unique_features = get_global_features_list(root_dir)\n",
    "        np.save('./unique_features.npy', unique_features)\n",
    "    print(\"Unique feature count: {}\".format(len(unique_features)))\n",
    "\n",
    "    # generate list of files & shuffle\n",
    "    fname_list = []\n",
    "    for rt,drs,fnames in os.walk(root_dir):\n",
    "        fname_list.extend(fnames)\n",
    "    shuffle(fname_list)\n",
    "\n",
    "    # identify which samples are a type of malware\n",
    "    malware_labels = []\n",
    "    malware_list = []\n",
    "    with open(family_file, 'r') as fi:\n",
    "        lines = [line for line in fi][1:]\n",
    "        for line in lines:\n",
    "            fname, family = line.strip().split(',')\n",
    "            malware_labels.append(family)\n",
    "            malware_list.append(fname)\n",
    "\n",
    "    # create label list\n",
    "    all_labels = []\n",
    "    for fname in fname_list:\n",
    "        try:\n",
    "            idx = malware_list.index(fname)\n",
    "            all_labels.append(malware_labels[idx])\n",
    "        except:\n",
    "            all_labels.append('benign')\n",
    "\n",
    "    # create generator\n",
    "    fpath_list = [os.path.join(root_dir, fname) for fname in fname_list]\n",
    "    #gen = data_generator(unique_features, fpath_list, 1000)\n",
    "    all_data = get_data_all(unique_features,fpath_list)\n",
    "    \n",
    "    # build samples matrix\n",
    "    samples = None\n",
    "    for i, batch in enumerate(gen):\n",
    "        if samples is None:\n",
    "            samples = batch\n",
    "        else:\n",
    "            samples = np.vstack([samples, batch])\n",
    "        #print(\"Progress: {}\".format(samples.shape[0] / len(fpath_list)), end='\\r', flush=True)\n",
    "        print(\"Progress: {}\".format(samples.shape[0] / len(fpath_list)))\n",
    "\n",
    "    return samples, all_labels, fname_list\n",
    "\n",
    "\n",
    "#samples, labels, fnames = load_data()\n",
    "#np.savez('drebin_proc.npz', labels=labels, X=samples, fnames=fnames)\n",
    "\n",
    "root_dir = 'feature_vectors'\n",
    "family_file = 'sha256_family.csv'\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "root_dir = 'feature_vectors'\n",
    "family_file = 'sha256_family.csv'\n",
    "\n",
    "\n",
    "if os.path.exists('./unique_features.npy'):\n",
    "    unique_features = np.load('./unique_features.npy')\n",
    "else:\n",
    "    # read through files and create the list of feature strings\n",
    "    unique_features = get_global_features_list(root_dir)\n",
    "    np.save('./unique_features.npy', unique_features)\n",
    "print(\"Unique feature count: {}\".format(len(unique_features)))\n",
    "\n",
    "# generate list of files & shuffle\n",
    "fname_list = []\n",
    "for rt,drs,fnames in os.walk(root_dir):\n",
    "    fname_list.extend(fnames)\n",
    "shuffle(fname_list)\n",
    "\n",
    "\n",
    "# identify which samples are a type of malware\n",
    "malware_labels = []\n",
    "malware_list = []\n",
    "with open(family_file, 'r') as fi:\n",
    "    lines = [line for line in fi][1:]\n",
    "    for line in lines:\n",
    "        fname, family = line.strip().split(',')\n",
    "        malware_labels.append(family)\n",
    "        malware_list.append(fname)\n",
    "print(len(malware_labels),len(malware_list))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# create label list\n",
    "all_labels = []\n",
    "for fname in fname_list:\n",
    "    try:\n",
    "        idx = malware_list.index(fname)\n",
    "        all_labels.append(malware_labels[idx])\n",
    "    except:\n",
    "        all_labels.append('benign')\n",
    "print(len(all_labels))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "start = time.time()\n",
    "# create generator\n",
    "fpath_list = [os.path.join(root_dir, fname) for fname in fname_list]\n",
    "all_data = get_data_all(unique_features,fpath_list)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "#print(f'time elapsed {(end-start)/60} mins.')\n",
    "print('time elapsed',((end-start)/60)  ,'mins.')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "np.savez('drebin_preproc.npz', X=all_data)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# build samples matrix\n",
    "samples = None\n",
    "for i, batch in enumerate(all_data):\n",
    "    if samples is None:\n",
    "        samples = batch\n",
    "    else:\n",
    "        samples = np.vstack([samples, batch])\n",
    "    print(\"Progress: {}\".format(samples.shape[0] / len(fpath_list)), end='\\r', flush=True)\n",
    "\n",
    "\n",
    "#samples, labels, fnames = load_data()\n",
    "#np.savez('drebin_proc.npz', labels=labels, X=samples, fnames=fnames)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
