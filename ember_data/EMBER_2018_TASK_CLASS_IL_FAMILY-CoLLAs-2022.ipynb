{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "import multiprocessing\n",
    "from ember_features import PEFeatureExtractor\n",
    "\n",
    "from datetime import datetime\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "import operator\n",
    "import copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emberdata_family_stat(data_dir):\n",
    "    #data_dir = \"../../ember/ember_data/2018_data/ember2018/\"\n",
    "    \n",
    "    raw_feature_paths_base_tr = [os.path.join(data_dir, \"train_features_{}.jsonl\".format(i)) for i in range(6)]\n",
    "    raw_feature_paths_base_te = [os.path.join(data_dir, \"test_features.jsonl\")]\n",
    "    raw_feature_paths = raw_feature_paths_base_tr + raw_feature_paths_base_te\n",
    "    #print(raw_feature_paths)\n",
    "\n",
    "    all_task_months = ['2018-01', '2018-02', '2018-03', '2018-04', '2018-05', '2018-06',\n",
    "                       '2018-07', '2018-08', '2018-09', '2018-10', '2018-11', '2018-12']\n",
    "    task_months = all_task_months\n",
    "\n",
    "    av_class_stats = {}\n",
    "    cnt_rows = 0\n",
    "    cnt_good_rows = 0\n",
    "    cnt_missing_rows = 0\n",
    "\n",
    "    for fp in raw_feature_paths:\n",
    "        #print(fp)\n",
    "        with open(fp, \"r\") as fin:\n",
    "            #print(fp)\n",
    "            for line in fin:\n",
    "                raw_features = json.loads(line)\n",
    "                #print(raw_features.keys())\n",
    "\n",
    "                if raw_features['appeared'] in task_months:\n",
    "                    if raw_features['label'] == 1: # and raw_features['avclass']\n",
    "                        #print(raw_features['label'], raw_features['avclass'])\n",
    "                        if raw_features['avclass'] not in av_class_stats.keys():\n",
    "                            av_class_stats[raw_features['avclass']] = 1\n",
    "                        else:\n",
    "                            av_class_stats[raw_features['avclass']] += 1\n",
    "                        cnt_rows += 1\n",
    "\n",
    "                    elif raw_features['label'] == 0:\n",
    "                        cnt_good_rows += 1\n",
    "                    elif raw_features['label'] == -1:\n",
    "                        #print(raw_features['label'], raw_features['avclass'])\n",
    "                        cnt_missing_rows += 1\n",
    "\n",
    "            #if cnt_rows == 2:\n",
    "            #    break\n",
    "    min_samples = 0\n",
    "\n",
    "    families_more_than_400_samples = {}\n",
    "\n",
    "    for k, v in av_class_stats.items():\n",
    "        if v >= min_samples and k != '':\n",
    "            families_more_than_400_samples[k] = v\n",
    "            \n",
    "    return families_more_than_400_samples, av_class_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(irow, raw_features_string, X_path, y_path, extractor, nrows):\n",
    "    \"\"\"\n",
    "    Vectorize a single sample of raw features and write to a large numpy file\n",
    "    \"\"\"\n",
    "    raw_features = json.loads(raw_features_string)\n",
    "    \n",
    "    feature_vector = extractor.process_raw_features(raw_features)\n",
    "\n",
    "    y = np.memmap(y_path, dtype=np.float32, mode=\"r+\", shape=nrows)\n",
    "    y[irow] = top_families_100_labels[raw_features[\"avclass\"]]\n",
    "\n",
    "    X = np.memmap(X_path, dtype=np.float32, mode=\"r+\", shape=(nrows, extractor.dim))\n",
    "    X[irow] = feature_vector\n",
    "\n",
    "\n",
    "def vectorize_unpack(args):\n",
    "    \"\"\"\n",
    "    Pass through function for unpacking vectorize arguments\n",
    "    \"\"\"\n",
    "    return vectorize(*args)\n",
    "\n",
    "\n",
    "\n",
    "def create_parent_folder(file_path):\n",
    "    if not os.path.exists(os.path.dirname(file_path)):\n",
    "        os.makedirs(os.path.dirname(file_path))\n",
    "        \n",
    "def raw_feature_iterator(file_paths, top_families):\n",
    "    \"\"\"\n",
    "    Yield raw feature strings from the inputed file paths\n",
    "    \"\"\"\n",
    "    all_task_months = ['2018-01', '2018-02', '2018-03', '2018-04', '2018-05', '2018-06',\n",
    "                   '2018-07', '2018-08', '2018-09', '2018-10', '2018-11', '2018-12']\n",
    "    \n",
    "    for path in file_paths:\n",
    "        with open(path, \"r\") as fin:\n",
    "            for line in fin:\n",
    "                raw_features = json.loads(line)\n",
    "                if raw_features['appeared'] in all_task_months:\n",
    "                    if raw_features['avclass'] != '':\n",
    "                        if raw_features['avclass'] in top_families and raw_features['label'] == 1:\n",
    "                            yield line\n",
    "\n",
    "\n",
    "def task_based_vectorize_subset(X_path, y_path, raw_feature_paths, top_families, extractor, nrows):\n",
    "    \"\"\"\n",
    "    Vectorize a subset of data and write it to disk\n",
    "    \"\"\"\n",
    "    # Create space on disk to write features to\n",
    "    X = np.memmap(X_path, dtype=np.float32, mode=\"w+\", shape=(nrows, extractor.dim))\n",
    "    y = np.memmap(y_path, dtype=np.float32, mode=\"w+\", shape=nrows)\n",
    "    del X, y\n",
    "\n",
    "    # Distribute the vectorization work\n",
    "    pool = multiprocessing.Pool()\n",
    "    argument_iterator = ((irow, raw_features_string, X_path, y_path, extractor, nrows)\n",
    "                         for irow, raw_features_string in enumerate(raw_feature_iterator(raw_feature_paths, top_families)))\n",
    "    #print(argument_iterator)\n",
    "    \n",
    "    \n",
    "    for _ in tqdm.tqdm(pool.imap_unordered(vectorize_unpack, argument_iterator), total=nrows):\n",
    "        pass\n",
    "    \n",
    "    #return argument_iterator\n",
    "\n",
    "        \n",
    "def task_num_rows(raw_feature_paths, top_families):\n",
    "    print(top_families)\n",
    "    all_task_months = ['2018-01', '2018-02', '2018-03', '2018-04', '2018-05', '2018-06',\n",
    "                   '2018-07', '2018-08', '2018-09', '2018-10', '2018-11', '2018-12']\n",
    "    cnt_rows = 0\n",
    "    for fp in raw_feature_paths:\n",
    "        #print(fp)\n",
    "        with open(fp, \"r\") as fin:\n",
    "            #print(fp)\n",
    "            for line in fin:\n",
    "                raw_features = json.loads(line)\n",
    "                if raw_features['appeared'] in all_task_months:\n",
    "                    if raw_features['avclass'] != '':\n",
    "                        if raw_features['avclass'] in top_families and raw_features['label'] == 1:\n",
    "                            cnt_rows += 1\n",
    "    return cnt_rows\n",
    "\n",
    "\n",
    "def create_task_based_vectorized_features(data_dir, save_dir, top_families, feature_version=2):\n",
    "    \"\"\"\n",
    "    Create feature vectors from raw features and write them to disk\n",
    "    \"\"\"\n",
    "    extractor = PEFeatureExtractor(feature_version)\n",
    "    \n",
    "    #print(f'Vectorizing {current_task} task data')\n",
    "    X_path = os.path.join(save_dir, \"X_train.dat\")\n",
    "    y_path = os.path.join(save_dir, \"y_train.dat\")\n",
    "    raw_feature_paths_base_tr = [os.path.join(data_dir, \"train_features_{}.jsonl\".format(i)) for i in range(6)]\n",
    "    raw_feature_paths_base_te = [os.path.join(data_dir, \"test_features.jsonl\")]\n",
    "    raw_feature_paths = raw_feature_paths_base_tr + raw_feature_paths_base_te\n",
    "    \n",
    "    \n",
    "    \n",
    "    nrows = task_num_rows(raw_feature_paths, top_families)\n",
    "    #print(nrows)\n",
    "    task_based_vectorize_subset(X_path, y_path, raw_feature_paths, top_families, extractor, nrows)\n",
    "    #argument_iterator = task_based_vectorize_subset(X_path, y_path, raw_feature_paths, task_months, extractor, nrows)\n",
    "    \n",
    "    #return argument_iterator\n",
    "\n",
    "def read_task_based_vectorized_features(save_dir, feature_version=2):\n",
    "    \"\"\"\n",
    "    Read vectorized features into memory mapped numpy arrays\n",
    "    \"\"\"\n",
    "\n",
    "    extractor = PEFeatureExtractor(feature_version)\n",
    "    ndim = extractor.dim\n",
    "    X_ = None\n",
    "    y_ = None\n",
    "\n",
    "\n",
    "    X_path = os.path.join(save_dir, \"X_train.dat\")\n",
    "    y_path = os.path.join(save_dir, \"y_train.dat\")\n",
    "    \n",
    "    y_ = np.memmap(y_path, dtype=np.float32, mode=\"r\")\n",
    "    N = y_.shape[0]\n",
    "    \n",
    "    X_ = np.memmap(X_path, dtype=np.float32, mode=\"r\", shape=(N, ndim))\n",
    "    \n",
    "    print(np.unique(y_))\n",
    "    \n",
    "    X, Y = X_, y_\n",
    "    \n",
    "    indx = [i for i in range(len(Y))]\n",
    "    random.shuffle(indx)\n",
    "\n",
    "    train_size = int(len(indx)*0.9)\n",
    "    trainset = indx[:train_size]\n",
    "    testset = indx[train_size:]\n",
    "\n",
    "    # Separate the training set\n",
    "    X_train = X[trainset]\n",
    "    Y_train = Y[trainset]\n",
    "\n",
    "    # Separate the test set\n",
    "    X_test = X[testset]\n",
    "    Y_test = Y[testset]\n",
    "    \n",
    "    \n",
    "    print(f'X_train {X_train.shape} Y_train {Y_train.shape} X_test {X_test.shape} Y_test {Y_test.shape}')\n",
    "    \n",
    "    print(f'saving files ...')\n",
    "    save_training_file = save_dir + 'XY_train.npz'\n",
    "    save_test_file = save_dir + 'XY_test.npz'\n",
    "    \n",
    "    np.savez(save_training_file, X_train=X_train, Y_train=Y_train)\n",
    "    np.savez(save_test_file, X_test=X_test, Y_test=Y_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_dir = \"../../ember/ember_data/2018_data/ember2018/\"\n",
    "data_dir = \"/home/mr6564/continual_research/ember/ember_data/2018_data/ember2018/\"\n",
    "\n",
    "families_more_than_400_samples, av_class_stats = get_emberdata_family_stat(data_dir)\n",
    "ordered_106_families = sorted(families_more_than_400_samples.items(),key=operator.itemgetter(1),reverse=True)\n",
    "\n",
    "\n",
    "ordered_100_families_keys_100 = []\n",
    "num_classes = 100\n",
    "\n",
    "cnt = 0\n",
    "for j in ordered_106_families:\n",
    "    #print(j[0], j[1])\n",
    "    cnt += 1\n",
    "    ordered_100_families_keys_100.append(j[0])\n",
    "    if cnt == num_classes:\n",
    "        break\n",
    "print(len(ordered_100_families_keys_100))\n",
    "\n",
    "    \n",
    "top_families_100_labels = {}\n",
    "\n",
    "for ind, fam in enumerate(ordered_100_families_keys_100):\n",
    "    top_families_100_labels[fam] = int(ind)\n",
    "    \n",
    "    \n",
    "all_task_months = ['2018-01', '2018-02', '2018-03', '2018-04', '2018-05', '2018-06',\n",
    "                   '2018-07', '2018-08', '2018-09', '2018-10', '2018-11', '2018-12']\n",
    "\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "#top_families = ordered_100_families_keys_100\n",
    "\n",
    "save_dir = '/home/mr6564/continual_research/top_classes_' + str(num_classes) + '/'\n",
    "\n",
    "create_parent_folder(save_dir)\n",
    "\n",
    "\n",
    "create_task_based_vectorized_features(data_dir, save_dir, ordered_100_families_keys_100, feature_version=2)\n",
    "read_task_based_vectorized_features(save_dir, feature_version=2)\n",
    "    \n",
    "    \n",
    "end_time = time.time()\n",
    "\n",
    "print(f'Elapsed time {(end_time - start_time)/60} mins.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
