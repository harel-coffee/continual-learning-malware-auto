{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "import multiprocessing\n",
    "from ember_features import PEFeatureExtractor\n",
    "\n",
    "def vectorize(irow, raw_features_string, X_path, y_path, extractor, nrows):\n",
    "    \"\"\"\n",
    "    Vectorize a single sample of raw features and write to a large numpy file\n",
    "    \"\"\"\n",
    "    raw_features = json.loads(raw_features_string)\n",
    "    \n",
    "    feature_vector = extractor.process_raw_features(raw_features)\n",
    "\n",
    "    y = np.memmap(y_path, dtype=np.float32, mode=\"r+\", shape=nrows)\n",
    "    y[irow] = raw_features[\"label\"]\n",
    "    \n",
    "\n",
    "    X = np.memmap(X_path, dtype=np.float32, mode=\"r+\", shape=(nrows, extractor.dim))\n",
    "    X[irow] = feature_vector\n",
    "\n",
    "\n",
    "def vectorize_unpack(args):\n",
    "    \"\"\"\n",
    "    Pass through function for unpacking vectorize arguments\n",
    "    \"\"\"\n",
    "    return vectorize(*args)\n",
    "\n",
    "\n",
    "\n",
    "def create_parent_folder(file_path):\n",
    "    if not os.path.exists(os.path.dirname(file_path)):\n",
    "        os.makedirs(os.path.dirname(file_path))\n",
    "        \n",
    "def raw_feature_iterator(file_paths, task_months):\n",
    "    \"\"\"\n",
    "    Yield raw feature strings from the inputed file paths\n",
    "    \"\"\"\n",
    "    for path in file_paths:\n",
    "        with open(path, \"r\") as fin:\n",
    "            for line in fin:\n",
    "                raw_features = json.loads(line)\n",
    "                if raw_features['appeared'] in task_months:\n",
    "                    yield line\n",
    "\n",
    "\n",
    "def task_based_vectorize_subset(X_path, y_path, raw_feature_paths, task_months, extractor, nrows):\n",
    "    \"\"\"\n",
    "    Vectorize a subset of data and write it to disk\n",
    "    \"\"\"\n",
    "    # Create space on disk to write features to\n",
    "    X = np.memmap(X_path, dtype=np.float32, mode=\"w+\", shape=(nrows, extractor.dim))\n",
    "    y = np.memmap(y_path, dtype=np.float32, mode=\"w+\", shape=nrows)\n",
    "\n",
    "    del X, y\n",
    "\n",
    "    # Distribute the vectorization work\n",
    "    pool = multiprocessing.Pool()\n",
    "    argument_iterator = ((irow, raw_features_string, X_path, y_path, extractor, nrows)\n",
    "                         for irow, raw_features_string in enumerate(raw_feature_iterator(raw_feature_paths, task_months)))\n",
    "    #print(argument_iterator)\n",
    "    \n",
    "    \n",
    "    for _ in tqdm.tqdm(pool.imap_unordered(vectorize_unpack, argument_iterator), total=nrows):\n",
    "        pass\n",
    "    \n",
    "    #return argument_iterator\n",
    "\n",
    "        \n",
    "def task_num_rows(raw_feature_paths, task_months):\n",
    "    cnt_rows = 0\n",
    "    \n",
    "    family_labels = []\n",
    "    \n",
    "    for fp in raw_feature_paths:\n",
    "        #print(fp)\n",
    "        with open(fp, \"r\") as fin:\n",
    "            #print(fp)\n",
    "            for line in fin:\n",
    "                raw_features = json.loads(line)\n",
    "                if raw_features['appeared'] in task_months:\n",
    "                    family_labels.append(raw_features['avclass'])\n",
    "                    cnt_rows += 1\n",
    "    return cnt_rows, family_labels\n",
    "\n",
    "\n",
    "def create_task_based_vectorized_features(data_dir, save_dir, current_task, task_months, feature_version=2):\n",
    "    \"\"\"\n",
    "    Create feature vectors from raw features and write them to disk\n",
    "    \"\"\"\n",
    "    extractor = PEFeatureExtractor(feature_version)\n",
    "    \n",
    "    print(f'Vectorizing {current_task} task data')\n",
    "    X_path = os.path.join(save_dir, \"X_train.dat\")\n",
    "    y_path = os.path.join(save_dir, \"y_train.dat\")\n",
    "    \n",
    "    #y_path_family_labels = os.path.join(save_dir, \"y_family_train.dat\")\n",
    "    \n",
    "    raw_feature_paths_base_tr = [os.path.join(data_dir, \"train_features_{}.jsonl\".format(i)) for i in range(6)]\n",
    "    raw_feature_paths_base_te = [os.path.join(data_dir, \"test_features.jsonl\")]\n",
    "    raw_feature_paths = raw_feature_paths_base_tr + raw_feature_paths_base_te\n",
    "    \n",
    "    nrows, family_labels = task_num_rows(raw_feature_paths, current_task)\n",
    "    #print(nrows)\n",
    "    \n",
    "    save_test_file = save_dir + 'task_family_labels.npz'\n",
    "    np.savez(save_test_file, family_labels=family_labels)\n",
    "    \n",
    "    \n",
    "    task_based_vectorize_subset(X_path, y_path, raw_feature_paths, current_task, extractor, nrows)\n",
    "    #argument_iterator = task_based_vectorize_subset(X_path, y_path, raw_feature_paths, task_months, extractor, nrows)\n",
    "    \n",
    "    #return argument_iterator\n",
    "\n",
    "def read_task_based_vectorized_features(save_dir, feature_version=2):\n",
    "    \"\"\"\n",
    "    Read vectorized features into memory mapped numpy arrays\n",
    "    \"\"\"\n",
    "\n",
    "    extractor = PEFeatureExtractor(feature_version)\n",
    "    ndim = extractor.dim\n",
    "    X_ = None\n",
    "    y_ = None\n",
    "\n",
    "\n",
    "    X_path = os.path.join(save_dir, \"X_train.dat\")\n",
    "    y_path = os.path.join(save_dir, \"y_train.dat\")\n",
    "    \n",
    "    y_ = np.memmap(y_path, dtype=np.float32, mode=\"r\")\n",
    "    N = y_.shape[0]\n",
    "    \n",
    "    X_ = np.memmap(X_path, dtype=np.float32, mode=\"r\", shape=(N, ndim))\n",
    "    \n",
    "    print(np.unique(y_))\n",
    "    \n",
    "    goodware_indices = []\n",
    "    malware_indices = []\n",
    "    \n",
    "    \n",
    "    for ind, i in enumerate(y_):\n",
    "        if i == 0:\n",
    "            goodware_indices.append(ind)\n",
    "        elif i == 1:\n",
    "            malware_indices.append(ind)\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    malware_goodware_indices = goodware_indices + malware_indices\n",
    "    \n",
    "    print(len(y_[malware_goodware_indices]), len(y_[goodware_indices]), len(y_[malware_indices]))\n",
    "    \n",
    "    \n",
    "    Y_family_labels_file = save_dir + 'task_family_labels.npz'\n",
    "    Y_fam_labels_ = np.load(Y_family_labels_file)\n",
    "    Y_fam_labels = Y_fam_labels_['family_labels']\n",
    "    \n",
    "    \n",
    "    X = X_[malware_goodware_indices]\n",
    "    Y = y_[malware_goodware_indices]\n",
    "    Y_families = Y_fam_labels[malware_goodware_indices]\n",
    "    \n",
    "    indx = [i for i in range(len(Y))]\n",
    "    random.shuffle(indx)\n",
    "\n",
    "    train_size = int(len(indx)*0.9)\n",
    "    trainset = indx[:train_size]\n",
    "    testset = indx[train_size:]\n",
    "\n",
    "    # Separate the training set\n",
    "    X_train = X[trainset]\n",
    "    Y_train = Y[trainset]\n",
    "    Y_family_train = Y_families[trainset]\n",
    "\n",
    "    # Separate the test set\n",
    "    X_test = X[testset]\n",
    "    Y_test = Y[testset]\n",
    "    Y_family_test = Y_families[testset]\n",
    "    \n",
    "    \n",
    "    print(f'X_train {X_train.shape} Y_train {Y_train.shape} Y_family_train {Y_family_train.shape}\\n X_test {X_test.shape} Y_test {Y_test.shape} \\n Y_family_test {Y_family_test.shape}')\n",
    "    \n",
    "    print(f'saving files ...')\n",
    "    save_training_file = save_dir + 'XY_train.npz'\n",
    "    save_test_file = save_dir + 'XY_test.npz'\n",
    "    \n",
    "    np.savez(save_training_file, X_train=X_train, Y_train=Y_train, Y_family_train = Y_family_train)\n",
    "    np.savez(save_test_file, X_test=X_test, Y_test=Y_test, Y_family_test = Y_family_test)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "all_task_months = ['2018-01', '2018-02', '2018-03', '2018-04', '2018-05', '2018-06',\n",
    "                   '2018-07', '2018-08', '2018-09', '2018-10', '2018-11', '2018-12']\n",
    "\n",
    "data_dir = \"../../../ember/ember_data/2018_data/ember2018/\"\n",
    "\n",
    "for task in range(0,len(all_task_months)):\n",
    "    start_time = time.time()\n",
    "    #task = 5 + task\n",
    "    current_task = all_task_months[task]\n",
    "    task_months = all_task_months[:task+1]\n",
    "    \n",
    "    \n",
    "    save_dir = '../../ember2018/month_based_processing_with_family_labels/' + str(current_task) + '/'\n",
    "    create_parent_folder(save_dir)\n",
    "    \n",
    "    print(f'Processing data for task {current_task}')\n",
    "    #print(current_task, task_months)\n",
    "    create_task_based_vectorized_features(data_dir, save_dir, current_task, task_months, feature_version=2)\n",
    "    read_task_based_vectorized_features(save_dir, feature_version=2)\n",
    "    \n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(f'Elapsed time {(end_time - start_time)/60} mins.')    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
